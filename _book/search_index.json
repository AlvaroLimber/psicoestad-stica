[["index.html", "Psicoestadística Prefacio Audiencia Estructura del libro", " Psicoestadística MSc. Alvaro Chirino Gutierrez 2025-10-30 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia Este libro esta destinado a la materia de Psicoestadística de la carrera de Psicología de la Universidad Católica Boliviana, sede La Paz. Estructura del libro El libro incluye los capítulos: Parte I Diseño estadístico Estadística descriptiva Parte II "],["diseño-estadístico.html", "1 Diseño estadístico 1.1 ¿Qué es la estadística? 1.2 Población Objetivo (PO) — Universo 1.3 Coberturas estadísticas 1.4 Unidades estadísticas 1.5 Fuentes de información 1.6 Enfoques para el análisis 1.7 Validez interna y externa 1.8 Sesgos posibles en la producción de datos 1.9 Variables y tipología 1.10 Medición", " 1 Diseño estadístico 1.1 ¿Qué es la estadística? La estadística ha sido definida de distintas maneras según el enfoque y el contexto de aplicación: Enfoque formal: la estadística es la ciencia que trata con la recolección, análisis, interpretación y presentación de datos (Wackerly, Mendenhall, and Scheaffer 2008). Enfoque aplicado: conjunto de métodos para planificar estudios, obtener datos y analizarlos con el propósito de tomar decisiones y resolver problemas (Montgomery and Runger 2014). Enfoque comunicativo: el arte de contar una historia con datos, enfatizando la comunicación clara y contextualizada de resultados (Gelman and Nolan 2017). Ejemplo: Analizar datos de un cuestionario de bienestar psicológico para comparar niveles de estrés entre grupos (p. ej., por semestre o género) y comunicar hallazgos que orienten intervenciones. 1.2 Población Objetivo (PO) — Universo Definición: \\[ U=\\{u_1, u_2, \\ldots, u_N\\} \\] Donde \\(N\\) es el tamaño del universo o la PO. Es el conjunto total de elementos o individuos sobre los que se quiere obtener información. Ejemplo: Todos los estudiantes matriculados en Psicología en Bolivia en 2025. 1.3 Coberturas estadísticas Cobertura geográfica (¿dónde?): territorio o ámbito institucional del estudio. Ejemplo: Universidades de La Paz. Cobertura temporal (¿cuándo?): periodo de referencia. Ejemplo: Marzo–junio de 2025. Cobertura temática (¿qué?): variables o fenómenos a medir. Ejemplo: Estrés académico, satisfacción con la carrera y rendimiento. 1.4 Unidades estadísticas Unidad de análisis/investigación: elemento sobre el que se analizan los datos. Ejemplo: Un estudiante. Unidad de información: quien proporciona los datos (puede coincidir o no con la unidad de análisis). Ejemplo: Un padre/madre como informante sobre hábitos de sueño de su hijo. Unidad agregada (dominio): grupo de unidades con características comunes para análisis comparativo. Ejemplo: Semestres, género, tipo de turno (mañana/tarde/noche). 1.5 Fuentes de información Censo: recuento completo de la población objetivo. Ejemplo: Censo de población y vivienda para estimar la distribución etaria. Encuesta por muestreo: medición de una parte de la población seleccionada mediante técnicas probabilísticas. Ejemplo: Encuesta nacional de salud mental en estudiantes. Estudio experimental: recolección de datos bajo condiciones controladas para probar hipótesis y estimar efectos. Ejemplo: Ensayo controlado sobre el impacto de mindfulness en ansiedad. Estudio observacional: registro sistemático sin manipulación de variables, en contextos naturales. Ejemplo: Observación del uso de redes sociales y su asociación con bienestar. Registro administrativo: datos reunidos por instituciones en el curso de sus funciones. Ejemplo: Historias clínicas o registros de atención psicológica. 1.6 Enfoques para el análisis Descriptivo: resume y presenta información observada. Ejemplo: Distribución de niveles de ansiedad y medidas de tendencia central. Relacional/diagnóstico: identifica asociaciones o perfiles entre variables. Ejemplo: Correlación entre horas de sueño y rendimiento académico. Predictivo: estima valores futuros o clasifica casos. Ejemplo: Modelo para predecir riesgo de depresión a partir de ítems de un test. Causal: evalúa el efecto de una intervención o exposición sobre un resultado, bajo supuestos de validez interna. Ejemplo: Efecto de una técnica de estudio en la memoria de corto plazo. 1.6.1 Fuentes de información × Enfoques de análisis Fuente / Enfoque Descriptivo Relacional/Diagnóstico Predictivo Causal Censo Sí Sí Sí Limitado* Encuesta por muestreo Sí Sí Sí Limitado** Estudio experimental Sí Sí Sí Sí Estudio observacional Sí Sí Sí Limitado*** Registro administrativo Sí Sí Sí Limitado*** * Sin manipulación, requiere supuestos fuertes para inferir causalidad. ** Posible mediante diseños cuasi-experimentales (p. ej., pareamiento, IV, DiD) si la calidad del diseño/datos lo permite. *** Depende del control de confusión y de la estrategia de identificación. 1.7 Validez interna y externa Validez interna: indica en qué medida los resultados de un estudio son correctos y válidos para el conjunto específico de unidades, condiciones y periodo observados. Una alta validez interna significa que la relación observada entre variables refleja de forma precisa el fenómeno estudiado en ese contexto, sin distorsiones por factores no controlados (Shadish, Cook, and Campbell 2002). No implica necesariamente que los resultados puedan extrapolarse a otros contextos. Ejemplo: Un experimento controlado en un laboratorio que evalúa el efecto de una intervención psicológica en un grupo concreto de estudiantes. Validez externa: se refiere a la capacidad de generalizar los resultados más allá de las condiciones del estudio, hacia otras poblaciones, contextos o momentos en el tiempo. Depende de la representatividad de la muestra, de la cobertura geográfica y temporal, y de la similitud entre el contexto del estudio y el contexto al que se quiere generalizar (Kish 1995; Shadish, Cook, and Campbell 2002). Ejemplo: Un estudio representativo a nivel nacional sobre depresión en adolescentes que permite inferir resultados para toda la población adolescente del país. 1.7.1 Fuentes de información × Tipos de validez Fuente / Validez Interna Externa Censo Limitada Alta (cobertura poblacional) Encuesta por muestreo Limitada Alta si el muestreo es representativo Estudio experimental Alta Variable (puede ser limitada por entornos artificiales) Estudio observacional Limitada Buena si el diseño y la muestra son adecuados Registro administrativo Variable Buena si la cobertura y la consistencia son altas 1.8 Sesgos posibles en la producción de datos En todo proceso estadístico pueden introducirse sesgos que afectan la validez de los resultados. Un sesgo es un error sistemático que produce estimaciones que se apartan consistentemente del valor real (Kish 1995; Groves et al. 2009). Sesgo de cobertura Ocurre cuando ciertas unidades de la población objetivo no tienen posibilidad de ser incluidas en el marco muestral (Kish 1995). Ejemplo: Encuesta en línea que excluye a personas sin acceso a internet. Sesgo de no respuesta Se produce cuando las personas seleccionadas para participar no responden, y su ausencia está relacionada con la variable de interés (Groves et al. 2009). Ejemplo: Personas con alta carga laboral no responden cuestionarios sobre estrés. Sesgo de medición Proviene de errores en la forma de recolectar la información, ya sea por el instrumento, el entrevistador o el encuestado (Groves et al. 2009). Ejemplo: Preguntas mal redactadas que inducen una respuesta. Sesgo de recuerdo Aparece cuando el encuestado no recuerda con precisión eventos pasados. Ejemplo: Preguntar cuántas veces visitó un psicólogo en el último año. Sesgo de selección Surge cuando la selección de participantes no es aleatoria y está asociada con el fenómeno de interés (Shadish, Cook, and Campbell 2002). Ejemplo: Seleccionar voluntarios para un experimento sobre motivación. Sesgo del observador Influencia que ejerce el investigador en la medición u observación del fenómeno. Ejemplo: Un psicólogo que evalúa de forma diferente a pacientes según su propia expectativa. Sesgo de procesamiento Introducido durante la codificación, digitación o análisis de los datos. Ejemplo: Errores de digitación que cambian el valor de una variable clave. 1.8.1 Fuentes de información × Sesgos más probables Fuente / Sesgo Cobertura No respuesta Medición Recuerdo Selección Observador Procesamiento Censo Medio Alto Medio Bajo Bajo Bajo Medio Encuesta por muestreo Alto Alto Medio Medio Medio Bajo Medio Estudio experimental Bajo Medio Medio Bajo Alto Medio Medio Estudio observacional Medio Medio Alto Medio Medio Alto Medio Registro administrativo Alto Bajo Medio Bajo Bajo Bajo Alto Notas: - Alto: el sesgo es frecuente y debe controlarse activamente. - Medio: el sesgo es posible y su impacto depende del diseño y la implementación. - Bajo: el sesgo es poco probable si se siguen protocolos adecuados. 1.9 Variables y tipología Cada unidad del universo estadístico \\(u_i \\in U\\) posee una serie de características o atributos que pueden observarse o medirse (Navarro, Foxcroft, and Faulkenberry 2025). En estadística, estas características se denominan variables y constituyen la base de la información que se analiza. Una variable es una propiedad que puede tomar distintos valores entre unidades de la población o a lo largo del tiempo (Kish 1995; Navarro, Foxcroft, and Faulkenberry 2025). Según su naturaleza y el tipo de análisis que permiten, las variables pueden clasificarse en: Cualitativas o categóricas: expresan atributos o categorías sin magnitud numérica intrínseca. Nominales: categorías sin orden (p. ej., sexo, tipo de terapia). Ordinales: categorías con un orden definido, pero sin distancias numéricas uniformes (p. ej., nivel educativo, escala de satisfacción). Cuantitativas o numéricas: expresan magnitudes numéricas y permiten operaciones aritméticas. Se subdividen en: Discretas: toman valores enteros, generalmente resultado de un conteo (p. ej., número de hijos, cantidad de sesiones de terapia). Continuas (decimales): pueden tomar cualquier valor en un rango, incluyendo decimales, resultado de una medición (p. ej., peso en kg, tiempo de reacción en segundos). La tipología de variables condiciona los métodos de análisis estadístico que pueden emplearse y afecta la validez de las inferencias (Navarro, Foxcroft, and Faulkenberry 2025). 1.10 Medición La medición es el proceso de asignar valores o categorías a las variables observadas en las unidades de análisis, siguiendo reglas previamente definidas y aceptadas (Navarro, Foxcroft, and Faulkenberry 2025; Stevens 1946). Su objetivo es representar un fenómeno de forma cuantitativa o cualitativa para su análisis estadístico, asegurando comparabilidad, consistencia y validez de la información recogida. Pasos clave en la medición: 1. Definir el constructo o fenómeno de interés (p. ej., ansiedad). 2. Seleccionar o desarrollar un instrumento de medición (cuestionario, escala, test, observación). 3. Determinar el nivel de medición de cada variable (nominal, ordinal, intervalo, razón) (Stevens 1946). 4. Estandarizar los procedimientos de recolección para minimizar sesgos y garantizar la calidad de los datos (Navarro, Foxcroft, and Faulkenberry 2025). Ejemplo: Medir el estrés académico en estudiantes universitarios utilizando una escala validada, aplicada bajo las mismas condiciones para todos los participantes y con un protocolo claro de codificación. "],["estadística-descriptiva.html", "2 Estadística descriptiva 2.1 Ordenación de los datos 2.2 Estadísticas para variables cualitativas 2.3 Medidas de resumen (Tendencia central) 2.4 Medidas de dispersión 2.5 Medidas de forma 2.6 Visualización univariante 2.7 Referencias", " 2 Estadística descriptiva La estadística descriptiva se ocupa de resumir, organizar y presentar la información de un conjunto de datos, sin pretender hacer inferencias sobre una población mayor (Navarro, Foxcroft, and Faulkenberry 2025). Sus herramientas permiten transformar datos brutos en información comprensible a través de tablas, medidas numéricas y representaciones gráficas (Moore, McCabe, and Craig 2013). 2.1 Ordenación de los datos El primer paso en la estadística descriptiva es la organización de los datos, es decir, cómo se disponen en una estructura clara que permita analizarlos (Navarro, Foxcroft, and Faulkenberry 2025). El enfoque más común es el enfoque tabular, que dispone la información en: Filas: cada fila corresponde a una unidad de análisis (por ejemplo, un estudiante, un paciente, una observación experimental). Columnas: cada columna corresponde a una variable (por ejemplo, edad, sexo, puntaje en una escala). Ejemplo: en un cuestionario aplicado a 5 estudiantes, las filas representan a cada estudiante y las columnas las variables recolectadas (edad, sexo, nivel de ansiedad). 2.2 Estadísticas para variables cualitativas Cuando los datos son cualitativos, los resúmenes más comunes son (Agresti and Finlay 2018): Conteos absolutos: número de casos en cada categoría. Frecuencias relativas (proporciones o porcentajes): relación de cada categoría respecto al total, expresada como proporción o en %. Tasas: razón entre un número de casos y la población expuesta en un periodo de tiempo (p. ej., tasa de deserción escolar). Razones: cociente entre dos frecuencias de distinta categoría (p. ej., razón hombre/mujer). Índices: valores compuestos que resumen múltiples indicadores en una sola medida. 2.3 Medidas de resumen (Tendencia central) Las medidas de tendencia central resumen en un solo valor el “centro” o valor típico de la distribución de los datos (Navarro, Foxcroft, and Faulkenberry 2025): Media (promedio aritmético): suma de todos los valores dividida entre el número de observaciones. \\[ \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i \\] Mediana: valor que divide el conjunto ordenado en dos mitades de igual tamaño. Útil cuando la distribución es asimétrica o existen valores atípicos. Moda: valor más frecuente en el conjunto de datos. Especialmente útil en variables cualitativas o discretas. Ejemplo: En los puntajes 5, 6, 6, 7, 10: Media = 6.8 Mediana = 6 Moda = 6 2.4 Medidas de dispersión Las medidas de dispersión indican el grado en que los datos se alejan del valor central. Son esenciales para complementar las medidas de tendencia central (Navarro, Foxcroft, and Faulkenberry 2025): Rango: diferencia entre el valor máximo y el mínimo. \\[ R = x_{\\max} - x_{\\min} \\] Varianza: medida promedio de la dispersión de los datos respecto a la media. \\[ s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2 \\] Desviación estándar: raíz cuadrada de la varianza, expresa la dispersión en las mismas unidades de la variable. \\[ s = \\sqrt{s^2} \\] Coeficiente de variación: Cociente entre la desviación estándar y la media, expresa la dispersión de los datos en términos relativos, mientras el valor se más cercano a cero existe menos dispoersión. \\[cv=\\frac{s}{\\bar{x}}\\] \\[cv\\%=\\frac{s}{\\bar{x}}*100\\] Ejemplo: En los datos 5, 6, 6, 7, 10: - Rango = 5 - Varianza = 3.7 - Desviación estándar ≈ 1.92 - Coeficiente de variación = 0.283 - \\(cv\\%=28.3\\) Notas: Las medidas de resumen y dispersión deben interpretarse en conjunto. Una media sin dispersión puede ser engañosa; la dispersión muestra la heterogeneidad de los datos. En investigación psicológica, es clave reportar ambas dimensiones para dar una imagen completa de los resultados. 2.5 Medidas de forma Además de la tendencia central y la dispersión, es importante describir la forma de la distribución de los datos. Las medidas de forma permiten evaluar si una variable se distribuye de manera simétrica o presenta concentraciones atípicas de valores (Navarro, Foxcroft, and Faulkenberry 2025; Moore, McCabe, and Craig 2013). 2.5.1 Asimetría (AS) La asimetría (AS) indica el grado en que la distribución se desvía de la simetría respecto a la media: AS &gt; 0 (positiva, sesgo a la derecha): la cola de la distribución es más larga hacia valores altos. Ejemplo: ingresos en una población, donde unos pocos individuos concentran valores muy elevados. AS &lt; 0 (negativa, sesgo a la izquierda): la cola es más larga hacia valores bajos. Ejemplo: edad de jubilación, donde la mayoría se concentra cerca del máximo. AS ≈ 0: distribución simétrica, como la normal. Fórmula (coeficiente de asimetría de Fisher): \\[ AS = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^3}{s^3} \\] donde \\(s\\) es la desviación estándar. 2.5.2 Curtosis (KU) La curtosis (KU) mide el grado de concentración de los valores en torno a la media (apuntamiento de la distribución): KU &gt; 0 (leptocúrtica): mayor concentración en el centro y colas más pesadas que la normal. KU = 0 (mesocúrtica): distribución similar a la normal. KU &lt; 0 (platicúrtica): menos concentración en el centro, distribución más achatada. Fórmula (exceso de curtosis de Fisher): \\[ KU = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^4}{s^4} - 3 \\] donde \\(KU = 0\\) corresponde a la normal. Notas didácticas AS y KU complementan las medidas de tendencia central y dispersión, aportando información sobre la forma. En investigación psicológica, ayudan a verificar supuestos de normalidad antes de aplicar pruebas paramétricas. Valores extremos de AS o KU pueden afectar fuertemente las inferencias estadísticas. En la práctica, es más informativo mostrar un histograma o un diagrama de caja que reportar Ejemplo aplicado: En un test de ansiedad aplicado a 200 estudiantes, se obtiene AS = 1.2 (asimetría positiva moderada) y KU = 1.5 (leptocúrtica). Esto indica que la mayoría de los estudiantes tienen niveles bajos a moderados, pero existe un pequeño grupo con niveles muy altos que genera colas largas y concentración en el centro. 2.6 Visualización univariante La visualización univariante muestra la distribución de una sola variable. Permite identificar patrones, valores típicos y posibles atípicos (Navarro, Foxcroft, and Faulkenberry 2025). 2.6.1 Para variables cualitativas Gráfico de barras: compara frecuencias absolutas o relativas de categorías. Gráfico de sectores (pie chart): representa proporciones como partes de un todo (menos recomendable en contextos académicos, pero popular en divulgación). 2.6.2 Para variables cuantitativas Histograma: muestra la distribución dividiendo el rango en intervalos. Polígono de frecuencias: línea que conecta las frecuencias de un histograma. Diagrama de caja (boxplot): resume mediana, cuartiles y valores atípicos. Densidad (kernel density plot): estimación suave de la distribución. Ejemplos Ejemplos web: Seeing Theory (Brown University): https://seeing-theory.brown.edu/ → Sección de Distributions e Exploratory Data Analysis, con histogramas, boxplots y densidades interactivos. Seeing Data: http://seeingdata.org/ → Centrado en visualización para ciencias sociales. 2.7 Referencias "],["estadística-teórica.html", "3 Estadística Teórica 3.1 Fundamentos de Probabilidad y Axiomas 3.2 Variable Aleatoria y Distribuciones 3.3 Muestreo, Teoremas Fundamentales y Estimación 3.4 Estimación de Parámetros 3.5 Tamaño de Muestra 3.6 Referencias", " 3 Estadística Teórica La estadística teórica proporciona los fundamentos matemáticos que sustentan la inferencia estadística. A través de la probabilidad, los modelos de distribución y los teoremas fundamentales, permite cuantificar la incertidumbre y estimar parámetros poblacionales a partir de muestras. En su aspecto teórico, permite establecer y entender la relación entre la Población Objetivo (P.O.) y la muestra (Navarro, Foxcroft, and Faulkenberry (2025); Moore, McCabe, and Craig (2013)). Habla sobre la probabilidad y la variable aleatoria, caracterizada por el azar, e implica las distribuciones muestrales, donde se conecta con la práctica y la realidad. 3.1 Fundamentos de Probabilidad y Axiomas La probabilidad es una medida de incertidumbre y constituye la base teórica de la inferencia estadística (Feller (1968)). 3.1.1 Experimento Aleatorio y Espacio Muestral Un experimento aleatorio es un proceso del cual NO se conoce el resultado antes de su realización. Los conceptos de aleatorio e incertidumbre están relacionados. Ejemplo en Psicología: Observar si un paciente abandona la terapia (\\(X=1\\)) o la continúa (\\(X=0\\)). El espacio muestral (\\(\\Omega\\)) es el conjunto de TODOS los resultados posibles que puede arrojar un experimento aleatorio. Espacio Muestral Discreto: Si \\(\\Omega\\) es finito o contablemente infinito. Ejemplo: Medir la calificación obtenida en un examen (valores posibles: 0, 1, 2, …, 10). \\(\\Omega = \\{0, 1, 2, \\dots, 10\\}\\). Espacio Muestral Continuo: Si \\(\\Omega\\) incluye todos los valores dentro de un intervalo (no contable). Ejemplo 3: Medir el tiempo de reacción (en segundos) de una persona a un estímulo visual. \\(\\Omega = [0, \\infty)\\). Un evento (\\(A\\)) es cualquier subconjunto del espacio muestral (\\(\\Omega\\)). Es la ocurrencia o no ocurrencia de un resultado específico o grupo de resultados. Cuando un evento es imposible, su probabilidad es igual a 0. Ejemplo de Evento (basado en Ejemplo 1, calificación): Evento \\(A\\): Aprobar el examen (obtener una calificación mayor a 5). \\(A = \\{6, 7, 8, 9, 10\\}\\). Evento \\(B\\): Obtener una calificación perfecta. \\(B = \\{10\\}\\). 3.1.2 Axiomas de la Probabilidad (Kolmogórov) Formalmente, un espacio probabilístico \\((\\Omega, \\mathcal{A}, P)\\) se define con base en los axiomas: No negatividad: La probabilidad de cualquier evento \\(A\\) debe estar entre 0 y 1. \\(P(A) \\ge 0\\). Unidad: La probabilidad del espacio muestral siempre es igual a 1. \\(P(\\Omega) = 1\\). Aditividad: Para una secuencia de eventos mutuamente excluyentes (\\(A_1, A_2, ...\\)), la probabilidad de su unión es la suma de sus probabilidades: \\[ P\\left(\\bigcup_i A_i\\right) = \\sum_i P(A_i) \\] 3.1.3 Enfoques Conceptuales de la Probabilidad Existen tres enfoques principales para interpretar la probabilidad, cada uno con implicaciones distintas en la aplicación de la estadística (Navarro, Foxcroft, and Faulkenberry (2025); Moore, McCabe, and Craig (2013)): 3.1.3.1 Probabilidad Teórica (Clásica) Deriva del razonamiento lógico o combinatorio y asume que todos los resultados posibles son igualmente probables (equiprobables). \\[ P(A) = \\frac{\\text{número de casos favorables}}{\\text{número total de casos posibles}} \\] Desarrollo: Este enfoque es útil principalmente para experimentos ideales, juegos de azar o situaciones muy controladas donde la simetría de los resultados es evidente. Es la definición más limitada en las ciencias empíricas como la psicología, pues rara vez se asume equiprobabilidad en los fenómenos de la vida real. Ejemplo en Psicología: Asumir la probabilidad de que un sujeto elija la Opción A o la Opción B en un test simple sin sesgos previos es \\(P(A) = 1/2\\). 3.1.3.2 Probabilidad Frecuentista (Empírica) Se define como el límite de la frecuencia relativa con que ocurre un evento en un número grande de repeticiones independientes del experimento. \\[ P(A) = \\lim_{n \\to \\infty} \\frac{f_A}{n} \\] Desarrollo: Es el enfoque más relevante y utilizado en la inferencia estadística clásica y en la investigación psicológica. La probabilidad se estima a partir de la observación empírica y la experiencia acumulada. La precisión de la estimación mejora a medida que aumenta el número de observaciones (\\(n \\to \\infty\\)), tal como lo establece la Ley de los Grandes Números. Ejemplo en Psicología: Si se observa que de 1,000 pacientes con un trastorno específico, 650 responden positivamente a un tipo de terapia, la probabilidad frecuentista de respuesta positiva es \\(P(\\text{Respuesta Positiva}) \\approx 650/1000 = 0.65\\). 3.1.3.3 Probabilidad Bayesiana (Subjetiva o Condicional) Interpreta la probabilidad como el grado de creencia racional sobre la ocurrencia de un evento, dado un conjunto de evidencias. * Desarrollo: Parte de una probabilidad inicial (a priori) que se ajusta y se actualiza mediante nueva evidencia (datos) para generar una Probabilidad Posterior más precisa. Se formaliza mediante el Teorema de Bayes. \\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\] 3.2 Variable Aleatoria y Distribuciones 3.2.1 Variable Aleatoria (v.a. = X) Una variable aleatoria (X) es una función que asigna a los eventos posibles una medida cuantificable. Es el soporte cuantitativo para las probabilidades. La distinción entre \\(X\\) (mayúscula) y \\(x\\) (minúscula) es crucial: * \\(X\\) (Mayúscula): Representa la variable aleatoria en sí, es decir, la regla o función que describe el conjunto de resultados posibles. Es la variable antes de que se realice el experimento. * \\(x\\) (Minúscula): Representa una realización o un valor específico que la variable aleatoria \\(X\\) puede tomar después de que el experimento ha ocurrido. Es un resultado concreto del recorrido. Ejemplo en Psicología: * Variable Aleatoria (\\(X\\)): El Puntaje de Ansiedad que obtendrá un sujeto en el próximo test. * Realización (\\(x\\)): El puntaje de 42 que obtuvo el sujeto al completar el test. La probabilidad se define como la posibilidad de que la variable aleatoria \\(X\\) tome un valor específico \\(x\\): \\(P(X = x)\\). 3.2.2 Distribuciones Discretas Las distribuciones discretas describen variables aleatorias cuyo recorrido es finito o contable (valores enteros, no continuos). 3.2.2.1 Experimento y Variable Aleatoria Bernoulli Un Experimento Bernoulli es el modelo de probabilidad más simple. Solo tiene dos resultados posibles mutuamente excluyentes: éxito (\\(X=1\\)) o fracaso (\\(X=0\\)). La probabilidad de éxito (\\(p\\)) se mantiene constante en cada ensayo. Fórmula (Función de Masa de Probabilidad): \\[ P(X = x) = p^x (1-p)^{1-x} \\quad \\text{para } x \\in \\{0, 1\\} \\] Ejemplo en Psicología: Determinar si un paciente abandona la terapia (éxito, \\(X=1\\)) o continúa (fracaso, \\(X=0\\)). Si la probabilidad histórica de abandono es \\(p=0.3\\), entonces \\(P(X=1) = 0.3\\) y \\(P(X=0) = 0.7\\). 3.2.2.2 Distribución y Experimento Binomial Un Experimento Binomial es una serie de \\(n\\) repeticiones independientes del experimento Bernoulli. La Distribución Binomial cuenta cuántos éxitos (\\(x\\)) ocurren en ese número fijo de intentos (\\(n\\)) con una probabilidad de éxito \\(p\\) constante. Fórmula (Función de Masa de Probabilidad): \\[ P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x} \\quad \\text{para } x \\in \\{0, 1, \\dots, n\\} \\] donde \\(\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\\) es el número de formas de obtener \\(k\\) éxitos en \\(n\\) intentos. Valor Esperado (Media) de la Binomial El Valor Esperado o Esperanza Matemática (\\(E[X]\\)) de una variable aleatoria es el promedio teórico a largo plazo si el experimento se repite un número infinito de veces. Para la distribución Binomial, representa el número esperado de éxitos. \\[ E[X] = n \\cdot p \\] Ejemplo Completo en Psicología: Una investigadora aplica un test de razonamiento moral con \\(n=20\\) ítems (preguntas de sí/no). Asume que la probabilidad de responder correctamente al azar es \\(p=0.5\\). Valor Esperado: El número esperado de respuestas correctas (por azar) es: \\[ E[X] = n \\cdot p = 20 \\cdot 0.5 = 10 \\] Se espera que, en promedio, una persona que responda al azar acierte 10 de las 20 preguntas. Cálculo de Probabilidad: Si la investigadora quiere saber la probabilidad de que un sujeto responda exactamente 15 preguntas correctamente al azar (\\(x=15\\)): \\[ P(X = 15) = \\binom{20}{15} (0.5)^{15} (1-0.5)^{20-15} \\] \\[ P(X = 15) = 15504 \\cdot (0.0000305) \\cdot (0.03125) \\approx 0.0148 \\] La probabilidad de obtener exactamente 15 aciertos al azar es muy baja (aproximadamente 1.48%). Esto permite a la investigadora inferir que un puntaje superior a 10 o 11 probablemente no se deba solo al azar. 3.2.3 Distribución Normal (Campana de Gauss) La Distribución Normal (o Campana de Gauss) es la distribución de probabilidad continua más importante en estadística inferencial. Describe variables que tienden a concentrarse simétricamente alrededor de un valor central, disminuyendo su frecuencia hacia los extremos. Fórmula (Función de Densidad de Probabilidad): \\[ f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\] Esta fórmula establece que la forma de la campana está completamente determinada por dos parámetros: la media (\\(\\mu\\)) (que define la posición central) y la desviación estándar (\\(\\sigma\\)) (que define la dispersión o altura). Propiedades Clave: Forma de Campana: Es perfectamente simétrica respecto a la media. En el punto central, la media, la mediana y la moda coinciden (\\(\\mu\\)). Determinada por \\(\\mu\\) y \\(\\sigma\\): Cambios en \\(\\mu\\) desplazan la curva horizontalmente, mientras que cambios en \\(\\sigma\\) la hacen más ancha (mayor dispersión) o más estrecha (menor dispersión). Regla Empírica (68-95-99.7): Un porcentaje predecible de los datos cae dentro de ciertos rangos respecto a la media: Aproximadamente el 68% de los datos se encuentra a \\(\\pm 1\\) desviación estándar (\\(\\mu \\pm 1\\sigma\\)). Aproximadamente el 95% de los datos se encuentra a \\(\\pm 2\\) desviaciones estándar (\\(\\mu \\pm 2\\sigma\\)). Aproximadamente el 99.7% de los datos se encuentra a \\(\\pm 3\\) desviaciones estándar (\\(\\mu \\pm 3\\sigma\\)). Ejemplo en Psicología: Las puntuaciones del Coeficiente Intelectual (CI) en grandes poblaciones siguen aproximadamente una distribución normal con \\(\\mu=100\\) y \\(\\sigma=15\\). Esto implica que, basándose en la regla empírica, el 95% de la población tiene un CI entre 70 y 130 (100 \\(\\pm\\) \\(2 \\times 15\\)). 3.3 Muestreo, Teoremas Fundamentales y Estimación 3.3.1 Muestreo e Inferencia Estadística El muestreo es la forma de extraer una muestra. Es fundamental porque el objetivo principal de la estadística es la Inferencia Estadística: el proceso de utilizar información limitada de una muestra para sacar conclusiones o hacer generalizaciones sobre una población más grande. 3.3.1.1 Concepto de Muestra y Población Población Objetivo (P.O.): Es el conjunto completo de elementos, individuos o eventos sobre los cuales el investigador desea obtener información y generalizar sus conclusiones. Muestra: Es un subconjunto (o “pedacito”) de la Población Objetivo que realmente se observa y mide en el estudio. La muestra se usa como sustituto de la P.O. por razones prácticas (costo, tiempo, imposibilidad de medir a todos). 3.3.1.2 El Rol del Muestreo Aleatorio Para que el proceso de inferencia sea válido, el muestreo debe ser aleatorio. * Muestreo Aleatorio: Incorpora el azar en la selección, asegurando que cada unidad de la P.O. tenga una probabilidad conocida (y usualmente igual) de ser incluida en la muestra. * Importancia: Un muestreo aleatorio minimiza el sesgo y es la única condición bajo la cual los principios de la probabilidad (como el Teorema del Límite Central) pueden aplicarse, permitiendo que la muestra sea representativa de la población. * Muestreo No Aleatorio: Se basa en la conveniencia, accesibilidad o juicio del investigador, lo que limita o anula la capacidad de generalizar estadísticamente a la P.O. 3.3.1.3 Conceptos de Estimación La Estimación es la actividad central de la inferencia, donde el investigador busca aproximar un valor desconocido de la población (el Parámetro) usando el dato conocido de la muestra (el Estimador). Concepto Definición Conexión con la Inferencia Símbolo Parámetro Una medida descriptiva calculada sobre la P.O.. Fijo, único, pero generalmente desconocido. Es el objetivo de la inferencia; lo que queremos descubrir. \\(\\theta\\) (ej. \\(\\mu\\)) Estimador La medida análoga calculada sobre la MUESTRA. Es una variable aleatoria que varía de muestra a muestra. Es la herramienta de la inferencia; lo que usamos para aproximar el parámetro. \\(\\hat{\\theta}\\) (ej. \\(\\bar{X}\\)) Ejemplo General: Nivel de Burnout en Psicólogos Población Objetivo (P.O.): Todos los psicólogos colegiados de una región. Muestra: Se selecciona aleatoriamente un grupo de 150 psicólogos para la encuesta. Parámetro (\\(\\mu\\)): La media real (desconocida) del nivel de burnout de todos los psicólogos de la región. Estimador (\\(\\bar{X}\\)): La media del nivel de burnout calculada a partir de la muestra de 150 psicólogos (Ej: \\(\\bar{X}=35.2\\)). El objetivo es usar \\(\\bar{X}=35.2\\) para inferir el valor de \\(\\mu\\). El proceso de inferencia busca evaluar la calidad y precisión del estimador (\\(\\hat{\\theta}\\)) en su intento de acercarse al verdadero valor del parámetro (\\(\\theta\\)). 3.3.2 Distribuciones Muestrales Una Distribución Muestral es la distribución de probabilidad de un estadístico (como la media \\(\\bar{X}\\) o la proporción \\(\\hat{p}\\)) cuando ese estadístico se calcula a partir de todas las posibles muestras de un tamaño fijo (\\(n\\)) que se pueden extraer de una población. Diferencia Clave: Una distribución de probabilidad describe cómo se comporta una variable individual (\\(X\\)), mientras que la distribución muestral describe cómo se comporta un estimador (\\(\\hat{\\theta}\\)) de muestra a muestra. 3.3.2.1 Distribución Muestral de la Media (\\(\\bar{X}\\)) La distribución muestral de la media es fundamental. Dos propiedades clave, derivadas de los teoremas fundamentales, definen su comportamiento: Media de la Distribución Muestral (\\(\\mu_{\\bar{X}}\\)): El promedio de todas las medias muestrales posibles es igual a la media poblacional. Esto asegura que la media muestral es un estimador insesgado. \\[ \\mu_{\\bar{X}} = \\mu \\] Error Estándar (\\(\\sigma_{\\bar{X}}\\)): La desviación estándar de la distribución muestral de la media se llama Error Estándar. Mide la variabilidad (la precisión) del estimador \\(\\bar{X}\\). \\[ \\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}} \\] Ejemplo: Gastos de Transporte Consideremos una población pequeña (\\(N=5\\)) de estudiantes universitarios con los siguientes gastos diarios de transporte (en Bs.): \\(X = \\{2, 4, 6, 8, 10\\}\\). Parámetro Poblacional (\\(\\mu\\)): \\[\\mu = \\frac{2 + 4 + 6 + 8 + 10}{5} = \\mathbf{6}\\] Muestreo (\\(n=3\\)): Se extraen todas las posibles muestras de tamaño \\(n=3\\). Existen \\(\\binom{5}{3} = 10\\) muestras posibles. Muestra Gastos (\\(X\\)) Media Muestral (\\(\\bar{X}\\)) 1 {2, 4, 6} 4.00 2 {2, 4, 8} 4.67 3 {2, 4, 10} 5.33 4 {2, 6, 8} 5.33 5 {2, 6, 10} 6.00 6 {2, 8, 10} 6.67 7 {4, 6, 8} 6.00 8 {4, 6, 10} 6.67 9 {4, 8, 10} 7.33 10 {6, 8, 10} 8.00 Verificación de la Media Muestral (\\(\\mu_{\\bar{X}}\\)): El promedio de las 10 medias muestrales es: \\[\\mu_{\\bar{X}} = \\frac{4.00 + \\dots + 8.00}{10} = \\frac{60}{10} = \\mathbf{6}\\] Conclusión del Ejemplo: Se confirma que la media de la distribución muestral (\\(\\mu_{\\bar{X}}=6\\)) es idéntica a la media poblacional (\\(\\mu=6\\)). Las medias de las muestras individuales (\\(\\bar{X}\\)) varían, pero su promedio exacto es el verdadero parámetro poblacional. El Error Estándar sería la desviación de estos 10 valores de \\(\\bar{X}\\) respecto a 6. 3.3.3 Ley de los Grandes Números (LGN) Establece que, a medida que el tamaño de la muestra (\\(n\\)) aumenta, la media muestral (\\(\\bar{X}\\)) se aproxima cada vez más a la media poblacional (\\(\\mu\\)) (Moore, McCabe, and Craig (2013)). \\[ \\lim_{n \\to \\infty} P(|\\bar{X} - \\mu| &lt; \\varepsilon) = 1 \\] Aplicación: Para tener una estimación precisa del nivel promedio de satisfacción laboral en una empresa, se necesitan encuestar a muchos empleados. Cuanto mayor sea \\(n\\), más cerca estará \\(\\bar{X}\\) del valor real \\(\\mu\\). 3.3.4 Teorema del Límite Central (TLC) Si se toma una muestra aleatoria suficientemente grande (\\(n&gt;20\\)) de cualquier población, la distribución de la media muestral (\\(\\bar{X}\\)) tenderá a ser aproximadamente normal (Navarro, Foxcroft, and Faulkenberry (2025)). La media muestral estandarizada sigue una distribución normal estándar: \\[ Z = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1) \\] 3.4 Estimación de Parámetros La Estimación es el proceso de aproximar un valor desconocido de la población (\\(\\theta\\)) usando el dato de la muestra (\\(\\hat{\\theta}\\)). 3.4.1 Estimación Puntual para la Media Se utiliza un valor único (el estadístico muestral) como la mejor aproximación del parámetro poblacional. \\[ \\hat{\\mu} = \\bar{X} \\] Ejemplo: Si la media de autoeficacia percibida en una muestra de 50 estudiantes es \\(\\bar{X} = 75\\), la estimación puntual de la media poblacional (\\(\\mu\\)) es 75. 3.4.2 Estimación por Intervalo de Confianza (IC) para la Media Es un rango de valores que probablemente contiene el parámetro poblacional con un nivel de confianza (\\(1-\\alpha\\)) determinado. Si \\(\\sigma\\) es conocida: \\[ IC_{(1-\\alpha)} = \\bar{X} \\pm Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\] Valores de \\(Z_{\\alpha/2}\\) para niveles de confianza comunes: 90% de Confianza: \\(Z_{\\alpha/2} = 1.645\\) 95% de Confianza: \\(Z_{\\alpha/2} = 1.96\\) 99% de Confianza: \\(Z_{\\alpha/2} = 2.576\\) (o 2.58) Ejemplo: Un IC al 95% para la media de estrés postraumático es \\((24.3, 27.1)\\). Esto significa que si repitiéramos el muestreo muchas veces, el 95% de los intervalos construidos contendrían la verdadera media poblacional. Si \\(\\sigma\\) no se conoce se la estima mediante la desviación estándar \\(s\\). 3.4.3 Pruebas de Hipótesis Una Prueba de Hipótesis es un procedimiento estadístico formalizado para tomar una decisión sobre un parámetro poblacional, al contrastar dos afirmaciones opuestas usando la evidencia de la muestra. Permite determinar si una creencia o conjetura (\\(\\hat{\\theta}\\)) está justificada por los datos. 3.4.3.1 Pasos de la Prueba de Hipótesis (General) El procedimiento general sigue una estructura de cuatro pasos: Formular Hipótesis (\\(H_0\\) y \\(H_a\\)): Se establecen la afirmación que se quiere probar (nula) y su alternativa. Establecer Nivel de Significación (\\(\\alpha\\)): Se define la probabilidad máxima que estamos dispuestos a aceptar de cometer un Error de Tipo I (generalmente \\(\\alpha=0.05\\) o \\(0.01\\)). Construir el Estadístico de Prueba: Se calcula el valor estandarizado (ej. \\(Z\\), \\(t\\)) que mide cuán lejos está el estimador muestral del valor propuesto en \\(H_0\\). Evaluar y Concluir: Se compara el estadístico de prueba con los valores críticos o se usa el \\(P\\)-valor para decidir si se rechaza o no se rechaza la Hipótesis Nula. 3.4.3.2 Formulación de Hipótesis Hipótesis Nula (\\(H_0\\)): Es la afirmación que se quiere probar y la que asume que no hay efecto, cambio o diferencia. Siempre incluye un signo de igualdad (Ej: \\(\\mu = 10\\)). Hipótesis Alternativa (\\(H_a\\)): Es la afirmación contraria a la nula. Es lo que se sospecha o se quiere demostrar. Prueba Bilateral: \\(H_a: \\mu \\ne 10\\) Prueba Unilateral a la Derecha: \\(H_a: \\mu &gt; 10\\) Prueba Unilateral a la Izquierda: \\(H_a: \\mu &lt; 10\\) 3.4.3.3 Errores en la Prueba de Hipótesis El objetivo es minimizar la probabilidad de cometer estos errores: Tipo de Error Definición Consecuencia (Riesgo Controlado) Error de Tipo I (\\(\\alpha\\)) Rechazar la hipótesis nula (\\(H_0\\)) cuando en realidad es verdadera. Falso Positivo. Se controla con el nivel de significación (\\(\\alpha\\)). Error de Tipo II (\\(\\beta\\)) No rechazar la hipótesis nula (\\(H_0\\)) cuando en realidad es falsa. Falso Negativo. Se relaciona con la potencia de la prueba (\\(1-\\beta\\)). 3.4.3.4 Prueba de Hipótesis para la Media (\\(\\mu\\)) Se utiliza para probar si la media poblacional (\\(\\mu\\)) es igual a un valor conocido ($ _0 $). El estadístico de prueba (generalmente \\(Z\\) o \\(t\\)) mide la distancia de la media muestral (\\(\\bar{X}\\)) a \\(\\mu_0\\) en términos de errores estándar. Estadístico de Prueba (para la media con \\(\\sigma\\) conocida): \\[ Z_{0} = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}} \\] Para pruebas bilaterales, la decisión; Se rechaza \\(H_0\\) cuando: \\[Z_0&gt;Z_{\\alpha/2} \\quad ó \\quad Z_0&lt;-Z_{\\alpha/2}\\] Los valores usuales para los \\(Z_{\\alpha/2}\\) son: al 10 de significanca: \\(Z_{\\alpha/2}=1.64\\) al 5 de significanca: \\(Z_{\\alpha/2}=1.96\\) al 1 de significanca: \\(Z_{\\alpha/2}=2.58\\) Para las pruebas unilaterales se utiliza el mismo estadístico de prueba \\(Z_0\\), con las siguientes hipótesis y región de rechazo. \\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu&gt;\\mu_0\\] Se rechaza \\(H_0\\) si \\(Z_0&gt;Z{\\alpha}\\). \\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu &lt; \\mu_0\\] Se rechaza \\(H_0\\) si \\(Z_0&lt;-Z{\\alpha}\\) Los valores usuales para los \\(Z_{\\alpha}\\) de pruebas unilaterales. \\(Z_{0.1}=1.28\\) \\(Z_{0.05}=1.64\\) \\(Z_{0.01}=2.33\\) 3.5 Tamaño de Muestra El cálculo del tamaño de muestra (\\(n\\)) es uno de los pasos iniciales más críticos en cualquier investigación que emplee muestreo, pues define la capacidad del estudio para obtener conclusiones válidas y precisas. 3.5.1 Relevancia de la Aleatoriedad y el Equilibrio Logístico Pertinencia del Cálculo: El cálculo del tamaño de muestra solo tiene sentido y validez cuando se utiliza un muestreo aleatorio. Si la muestra es no aleatoria (por conveniencia), la inferencia estadística está comprometida, independientemente del tamaño calculado. Balance Estadístico vs. Logístico: Determinar \\(n\\) requiere un equilibrio entre: Necesidad Estadística: El tamaño mínimo requerido para alcanzar la precisión deseada (reducir el error estándar o aumentar la potencia). Factibilidad Logística: Los recursos disponibles (tiempo, costo, personal) para recolectar los datos. 3.5.2 Cálculo para Fines de Inferencia Descriptiva (Margen de Error) Para la inferencia descriptiva, el cálculo se basa en el margen de error (\\(e\\)) o precisión deseada, que es la máxima diferencia aceptable entre el estimador muestral (\\(\\hat{\\theta}\\)) y el parámetro poblacional (\\(\\theta\\)). 3.5.2.1 Cálculo para la Media Se utiliza cuando se desea estimar el promedio (\\(\\mu\\)). \\[ n = \\frac{Z^2 \\cdot \\sigma^2}{e^2}=\\left(\\frac{z*\\sigma}{e}\\right)^2 \\] Donde: * \\(Z\\): Valor crítico asociado al nivel de confianza deseado. * \\(\\sigma^2\\): Varianza de la población (a menudo estimada mediante estudios piloto o valores conservadores). * \\(e\\): Margen de error o precisión máxima aceptable. 3.5.2.2 Cálculo para la Proporción Se utiliza cuando se desea estimar un porcentaje o proporción (\\(p\\)). \\[ n = \\frac{Z^2 \\cdot p(1-p)}{e^2} \\] Donde: * \\(Z\\): Valor crítico asociado al nivel de confianza. * \\(p\\): Proporción poblacional esperada (si se desconoce, se usa \\(p=0.5\\) para maximizar el tamaño de muestra. * \\(e\\): Margen de error expresado como proporción, usualmente \\(0.05\\) 3.5.3 Cálculo para Fines de Inferencia Exploratoria (Potencia) Para la inferencia exploratoria (pruebas de hipótesis), el tamaño de muestra se calcula para asegurar la potencia adecuada, minimizando el Error de Tipo II (\\(\\beta\\)). Este cálculo debe considerar la magnitud del efecto. 3.5.3.1 Cálculo Básico para la Media (Potencia) El cálculo se realiza para detectar una diferencia mínima esperada (\\(\\mu_1 - \\mu_0\\), la magnitud del efecto) con una potencia deseada (\\(1-\\beta\\)) y un nivel de significación \\(\\alpha\\). \\[ n = \\frac{(Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot \\sigma^2}{(\\mu_1 - \\mu_0)^2} \\] Donde: \\(Z_{\\alpha/2}\\): Valor Z asociado al nivel de significación (\\(\\alpha\\)). \\(Z_{\\beta}\\): Valor Z asociado a la probabilidad del Error de Tipo II (\\(\\beta\\)). \\((\\mu_1 - \\mu_0)\\): La magnitud mínima del efecto (diferencia clínicamente significativa) que se espera detectar. 3.6 Referencias "],["herramientas-estadísticas.html", "4 Herramientas estadísticas 4.1 Análisis de datos categóricos: prueba \\(\\chi^2\\) 4.2 Comparación de dos medias: pruebas z, t y no paramétricas 4.3 Correlación y regresión lineal 4.4 Comparar varias medias: ANOVA de una vía 4.5 ANOVA factorial y ANCOVA", " 4 Herramientas estadísticas Recordar los pasos en las pruebas de hipótesis Formular Hipótesis (\\(H_0\\) y \\(H_a\\)) Establecer Nivel de Significación (\\(\\alpha\\)) Construir el Estadístico de Prueba Evaluar y Concluir El p-valor es una medida que permite evaluar la evidencia en contra de una hipótesis nula. Representa la probabilidad de obtener un resultado tan extremo (o más) que el observado, asumiendo que la hipótesis nula es verdadera. Un p-valor pequeño indica que los datos son poco compatibles con la hipótesis nula y sugiere evidencia a favor de la alternativa. 4.1 Análisis de datos categóricos: prueba \\(\\chi^2\\) Cuando las variables son categóricas (nominales u ordinales), la información se resume en términos de frecuencias. La prueba chi-cuadrado (\\(\\chi^2\\)) permite evaluar si las frecuencias observadas difieren de las que esperaríamos bajo una hipótesis teórica. Es una de las herramientas más utilizadas en psicología para analizar respuestas en categorías o preferencias (Navarro, Foxcroft, and Faulkenberry 2025). 4.1.1 Prueba de bondad de ajuste Evalúa si las frecuencias observadas en una variable categórica se ajustan a un modelo teórico o distribución esperada. Hipótesis \\[ H_0: \\text{Las frecuencias observadas se ajustan a las frecuencias esperadas.}\\\\ H_1: \\text{Las frecuencias observadas difieren significativamente de las esperadas.} \\] Estadístico \\[ \\chi^2_0 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i} \\] donde: - \\(O_i\\): frecuencia observada, - \\(E_i\\): frecuencia esperada bajo \\(H_0\\), - \\(k\\): número de categorías. Los grados de libertad se calculan como \\(gl = k - 1\\). El valor de \\(\\chi^2\\) se compara con la distribución chi-cuadrado con esos grados de libertad. Decisión Se rechaza la H0 si: \\[\\chi^2_0&gt; \\chi^2_{\\alpha, k-1}\\] 4.1.1.1 Ejemplo Un psicólogo evalúa si las preferencias por cuatro tipos de música (pop, rock, clásica, jazz) se distribuyen uniformemente. Las frecuencias observadas son (30, 25, 20, 25) y las esperadas, si no hubiera preferencia, serían iguales (25 cada una): \\[ \\chi^2 = \\frac{(30-25)^2}{25} + \\frac{(25-25)^2}{25} + \\frac{(20-25)^2}{25} + \\frac{(25-25)^2}{25} = 2 \\] Con \\(gl = 3\\) y \\(\\alpha = 0.05\\), el valor crítico es 7.815. Como 2 &lt; 7.815, no se rechaza \\(H_0\\) → no hay evidencia de diferencia significativa en las preferencias. Usando el p-valor (JASP) Si \\(p &lt; \\alpha\\), se concluye que las frecuencias no se ajustan al modelo teórico. Si \\(p \\ge \\alpha\\), las diferencias observadas son compatibles con la hipótesis nula. La prueba evalúa el grado de ajuste, no la dirección del efecto (Navarro, Foxcroft, and Faulkenberry 2025). 4.1.2 Prueba de independencia Evalúa si dos variables categóricas están asociadas o son independientes. Se trabaja con una tabla de contingencia de tamaño \\(r \\times c\\). Hipótesis \\[ H_0: \\text{Las variables son independientes.}\\\\ H_1: \\text{Existe asociación entre las variables.} \\] Estadístico de Prueba \\[ \\chi^2_0 = \\sum_{i=1}^{r}\\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\] Donde \\[ E_{ij} = \\frac{(F_{i\\cdot})(F_{\\cdot j})}{n} \\] donde \\(F_{i\\cdot}\\) y \\(F_{\\cdot j}\\) son los totales marginales por fila y columna, respectivamente, y \\(n\\) es el total general. Los grados de libertad son: \\(gl = (r-1)(c-1)\\) Decisión Se rechaza H0 si: \\[\\chi^2_0&gt; \\chi^2_{\\alpha, (r-1)(c-1)}\\] 4.1.2.1 Ejemplo Se evalúa si el género (hombre/mujer) está asociado con la preferencia de horario (mañana/tarde) en 100 estudiantes: Mañana Tarde Total Hombre 20 30 50 Mujer 10 40 50 Total 30 70 100 Frecuencia esperada para la celda (Hombre, Mañana): \\[ E_{11} = \\frac{(50)(30)}{100} = 15 \\] El estadístico \\(\\chi^2\\) calculado es 6.67 con \\(gl = 1\\). El valor crítico a \\(\\alpha = 0.05\\) es 3.84 → se rechaza \\(H_0\\), concluyendo que el género y la preferencia de horario están asociados. 4.1.3 Consideraciones y recomendaciones prácticas Cada frecuencia esperada debe ser al menos 5 para la validez de la prueba. En tablas 2×2 puede aplicarse la corrección de continuidad de Yates. Si hay celdas pequeñas, se puede usar la prueba exacta de Fisher. La prueba \\(\\chi^2\\) no indica la magnitud de la asociación, solo su existencia. Para cuantificarla, se pueden usar medidas derivadas como el V de Cramer o el coeficiente de contingencia (C). Ejemplo interpretativo: En un estudio sobre estilos de afrontamiento y género, se obtiene \\(\\chi^2(2) = 10.5\\), p &lt; .01. Esto sugiere una relación significativa entre el género y el estilo de afrontamiento. 4.1.4 Resumen comparativo Tipo de prueba Propósito principal Hipótesis nula (\\(H_0\\)) Grados de libertad Ejemplo típico Bondad de ajuste Contrastar una distribución observada con una esperada Los datos siguen la distribución teórica \\(k - 1\\) Preferencia por tipos de música Independencia Evaluar asociación entre dos variables categóricas Las variables son independientes \\((r-1)(c-1)\\) Asociación entre género y estilo de afrontamiento La prueba \\(\\chi^2\\) es una herramienta flexible y esencial para el análisis de datos categóricos. Su interpretación debe complementarse con medidas de tamaño del efecto y con la inspección de las tablas de contingencia (Navarro, Foxcroft, and Faulkenberry 2025; Moore, McCabe, and Craig 2013). Nota: Cramér’s V Mide el nivel de asociación entre dos variables cualitativas. \\[V=\\sqrt{\\frac{\\chi^2_0}{n*(k-1)}}\\] Donde \\(k\\) es el mínimo entre el número de clases de la variable fila y la variable columna. Sobre la lectura: Si \\(V\\rightarrow 0\\) Asociación débil Si \\(V\\rightarrow 1\\) Asociación fuerte 4.2 Comparación de dos medias: pruebas z, t y no paramétricas 4.3 Correlación y regresión lineal 4.4 Comparar varias medias: ANOVA de una vía 4.5 ANOVA factorial y ANCOVA Agresti, Alan, and Barbara Finlay. 2018. Statistical Methods for the Social Sciences. 5th ed. Boston: Pearson. Cohen, Jacob. 1994. “The Earth Is Round (p &lt; .05).” American Psychologist 49 (12): 997–1003. Feller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. New York: John Wiley &amp; Sons. Gelman, Andrew, and Deborah Nolan. 2017. Teaching Statistics: A Bag of Tricks. 2nd ed. Oxford: Oxford University Press. Groves, Robert M., Floyd J. Fowler, Mick P. Couper, James M. Lepkowski, Eleanor Singer, and Roger Tourangeau. 2009. Survey Methodology. 2nd ed. Hoboken, NJ: Wiley. Kish, Leslie. 1995. Survey Sampling. New York: John Wiley &amp; Sons. Montgomery, Douglas C., and George C. Runger. 2014. Applied Statistics and Probability for Engineers. 6th ed. Hoboken, NJ: John Wiley &amp; Sons. Moore, David S., George P. McCabe, and Bruce A. Craig. 2013. Introduction to the Practice of Statistics. 7th ed. New York: W. H. Freeman. Navarro, Danielle J., David R. Foxcroft, and Thomas J. Faulkenberry. 2025. “Learning Statistics with JASP: A Tutorial for Psychology Students and Other Beginners.” https://learnstatswithjasp.com. Shadish, William R., Thomas D. Cook, and Donald T. Campbell. 2002. Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Boston: Houghton Mifflin. Stevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. Wackerly, Dennis D., William Mendenhall, and Richard L. Scheaffer. 2008. Mathematical Statistics with Applications. 7th ed. Belmont, CA: Thomson Brooks/Cole. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
